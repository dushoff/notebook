## Binomial tests 

The standard frequentist binomial test can be conceptualized by a permutation test. This conceptualization can be expanded to more difficult problems (like risk ratios in a 2x2 table).

Imagine that we have a group of people who were given a vaccine. 7 showed a positive serological response, and 10 did not. What is our confidence interval for the proportion of vaccine responders in the population from which they were drawn?

The standard approach takes each possible value of the underlying proportion $p$, and calculates the (one-tailed) probability of observing a result as or more "extreme" (different from the expectation) as the observed data. If the probability is $> \alpha/2$, then the hypothesized $p$ is in the confidence interval.

```{r}
x <- 7
n <- 17
seed <- 946
t <- binom.test(x, n)
print(t)
```

In practice, this probability is calculated using the binomial formula. We can confirm this by taking the values kicked out by the test above and testing them using the binomial formula.


```{r}
print(lim <- t$conf.int)
print(pbinom(size=n
	, q=c(x, x-1) 
	, prob=c(lim[[2]], lim[[1]]))
)
```

## Permutation binomials 

If we didn't know the binomial formula, we could do the same thing by simulating (it's not clear what the point is, since we ''do'' know the binomial formula, but I'll get back to that). For each hypothetical value of $p$, we could simulate the process of randomly deciding who showed a positive response 10,000 or so times, and get an accurate estimate of the desired probability. 

Besides having no clear point, this idea seems impractical, since we want to do these thousands of calculations for each possible value of $p$.  We can solve this by re-conceptualizating the simulation. A standard way to think about simulating a binomial outcome is to compare the success probability $p$ to a uniform random number $U_i$ (from 0-1) for each individual $i$: if your random number $U_i < p$, then you have a positive vaccine response, otherwise not. But we can choose the $U_i$ values ''first'', and then decide what $p$ is. This will have the same result -- and we can essentially test every value of $p$ at once, since once we've chosen the values of $U_i$, we know which values of $p$ are consistent with, or else too large or too small for, our observed result.

So, for example, if we chose the numbers below:


```{r}
set.seed(seed)
print(sort(runif(n)))
```

our observation (7 "successes") would be consistent with $p$ between the seventh value (0.559) and the eighth (0.638). 

## A permutation binomial test 

If we do this thousands of times, and take the 0.025 quantile of the lower limit and the 0.975 quantile of the upper limit, we should have a very good approximation of the standard binomial test.

```{r}
sprob <- function(x, n){
	vals <- c(0, sort(runif(n)), 1)
	return(c(low=vals[x+1], high=vals[x+2]))
}

btest <- function(x, n, reps=5000, P=0.025){
	r <- replicate(reps, sprob(x=x, n=n))
	offset <- floor(reps*P)
	return(c(
		low=sort(r["low", ])[offset],
		high=sort(r["high", ])[1+reps-offset]
	))
}
```


```{r}
set.seed(seed)
btest(x, n)
```

So now we have an interesting conceptualization, and also a reasonably fast, accurate method of estimating binomial confidence intervals which could be used in the event that all of the copies of the binomial formula were destroyed in some sort of freak accident.

## A permutation risk-ratio test 

The point of course, is that we can easily use the same sampling approach to make valid tests for other quantities that can be estimated from binomial trials. The key one is the risk ratio.

